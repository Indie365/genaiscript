import { Fragment, PromptTemplate } from "./ast"
import { assert, normalizeFloat, normalizeInt } from "./util"
import { MarkdownTrace } from "./trace"
import type { ChatCompletionMessageParam } from "openai/resources"
import { isCancelError } from "./error"
import { estimateTokens } from "./tokens"
import { DEFAULT_MODEL, DEFAULT_TEMPERATURE, SYSTEM_FENCE } from "./constants"
import { PromptImage, renderPromptNode } from "./promptdom"
import { RunTemplateOptions, createPromptContext } from "./promptcontext"
import { evalPrompt } from "./evalprompt"

const defaultTopP: number = undefined
const defaultSeed: number = undefined
const defaultMaxTokens: number = undefined

export interface FragmentTransformResponse extends PromptGenerationOutput {
    /**
     * Summary of the output generated by the LLM
     */
    summary?: string

    /**
     * The env variables sent to the prompt
     */
    vars: Partial<ExpansionVariables>

    /**
     * Expanded prompt text
     */
    prompt: ChatCompletionMessageParam[]

    /**
     * Zero or more edits to apply.
     */
    edits: Edits[]

    /**
     * Parsed source annotations
     */
    annotations: Diagnostic[]

    /**
     * ChangeLog sections
     */
    changelogs: string[]

    /**
     * MD-formatted trace.
     */
    trace: string

    /**
     * Error message if any
     */
    error?: unknown

    /**
     * Run label if provided
     */
    label?: string

    /**
     * GenAIScript version
     */
    version: string
}

async function callExpander(
    r: PromptTemplate,
    vars: ExpansionVariables,
    trace: MarkdownTrace,
    options: RunTemplateOptions
) {
    const model = r.model || DEFAULT_MODEL
    const ctx = createPromptContext(r, vars, trace, options, model)

    let success = true
    let logs = ""
    let text = ""
    let images: PromptImage[] = []
    let schemas: Record<string, JSONSchema> = {}
    let functions: ChatFunctionCallback[] = []
    let fileMerges: FileMergeHandler[] = []
    let outputProcessors: PromptOutputProcessorHandler[] = []

    try {
        await evalPrompt(ctx, r, {
            sourceMaps: true,
            logCb: (msg: any) => {
                logs += msg + "\n"
            },
        })
        const node = ctx.node
        const {
            prompt,
            images: imgs,
            errors,
            schemas: schs,
            functions: fns,
            fileMerges: fms,
            outputProcessors: ops,
        } = await renderPromptNode(model, node, { trace })
        text = prompt
        images = imgs
        schemas = schs
        functions = fns
        fileMerges = fms
        outputProcessors = ops
        if (errors) for (const error of errors) trace.error(``, error)
    } catch (e) {
        success = false
        if (isCancelError(e)) {
            trace.log(`cancelled: ${(e as Error).message}`)
            // null is cancelled
            success = null
        } else {
            const m = /at eval.*<anonymous>:(\d+):(\d+)/.exec(e.stack)
            const info = m ? ` at prompt line ${m[1]}, column ${m[2]}` : ""
            trace.error(info, e)
        }
    }

    return {
        logs,
        success,
        text,
        images,
        schemas,
        functions,
        fileMerges,
        outputProcessors,
    }
}

export async function expandTemplate(
    template: PromptTemplate,
    fragment: Fragment,
    options: RunTemplateOptions,
    env: ExpansionVariables,
    trace: MarkdownTrace
) {
    const cancellationToken = options?.cancellationToken
    const { jsSource } = template

    trace.detailsFenced("üìÑ spec", env.spec.content, "markdown")
    trace.startDetails("üíæ script")

    trace.startDetails("üß¨ prompt")
    trace.detailsFenced("üìì script source", template.jsSource, "js")
    const prompt = await callExpander(template, env, trace, options)
    const expanded = prompt.text
    const images = prompt.images
    const schemas = prompt.schemas
    const functions = prompt.functions
    const fileMerges = prompt.fileMerges
    const outputProcessors = prompt.outputProcessors

    let success = prompt.success
    if (success === null)
        // cancelled
        return { success }

    const model =
        options.model ?? env.vars["model"] ?? template.model ?? DEFAULT_MODEL
    const temperature =
        options.temperature ??
        normalizeFloat(env.vars["temperature"]) ??
        template.temperature ??
        DEFAULT_TEMPERATURE
    const topP =
        options.topP ??
        normalizeFloat(env.vars["top_p"]) ??
        template.topP ??
        defaultTopP
    const max_tokens =
        options.maxTokens ??
        normalizeInt(env.vars["maxTokens"]) ??
        template.maxTokens ??
        defaultMaxTokens
    let seed =
        options.seed ??
        normalizeInt(env.vars["seed"]) ??
        template.seed ??
        defaultSeed
    if (seed !== undefined) seed = seed >> 0

    let responseType = template.responseType

    if (prompt.logs?.length) trace.details("üìù console.log", prompt.logs)
    trace.itemValue(`model`, model)
    trace.itemValue(`tokens`, estimateTokens(model, expanded))
    trace.itemValue(`temperature`, temperature)
    trace.itemValue(`top_p`, topP)
    trace.itemValue(`max tokens`, max_tokens)
    trace.fence(expanded, "markdown")
    trace.endDetails()

    if (cancellationToken?.isCancellationRequested) return { success: null }

    let systemText = ""
    const systems = (template.system ?? []).slice(0)
    if (template.system === undefined) {
        systems.push("system")
        systems.push("system.explanations")
        // select file expansion type
        if (/diff/i.test(jsSource)) systems.push("system.diff")
        else if (/changelog/i.test(jsSource)) systems.push("system.changelog")
        else systems.push("system.files")
        if (/annotations?/i.test(jsSource)) systems.push("system.annotations")
        if (/defschema/i.test(jsSource)) systems.push("system.schema")
    }
    for (let i = 0; i < systems.length && success; ++i) {
        if (cancellationToken?.isCancellationRequested) return { success: null }

        let systemTemplate = systems[i]
        let system = fragment.file.project.getTemplate(systemTemplate)
        if (!system) {
            if (systemTemplate) trace.error(`\`${systemTemplate}\` not found\n`)
            if (i > 0) continue
            systemTemplate = "system"
            system = fragment.file.project.getTemplate(systemTemplate)
            assert(!!system)
        }

        trace.startDetails(`üëæ ${systemTemplate}`)
        const sysr = await callExpander(system, env, trace, options)
        const sysex = sysr.text
        success = sysr.success
        if (success === null) return { success }
        else if (!success) break
        if (sysr.images) images.push(...sysr.images)
        if (sysr.schemas) Object.assign(schemas, sysr.schemas)
        if (sysr.functions) functions.push(...sysr.functions)
        if (sysr.fileMerges) fileMerges.push(...sysr.fileMerges)
        if (sysr.outputProcessors) outputProcessors.push(...outputProcessors)
        systemText += SYSTEM_FENCE + "\n" + sysex + "\n"

        responseType = responseType ?? system.responseType
        if (sysr.logs?.length) trace.details("üìù console.log", sysr.logs)
        trace.item(
            `tokens: ${estimateTokens(model || template.model || DEFAULT_MODEL, sysex)}`
        )

        trace.fence(system.jsSource, "js")
        trace.heading(3, "expanded")
        trace.fence(sysex, "markdown")
        trace.endDetails()
    }

    {
        trace.startDetails("‚öôÔ∏è configuration")
        trace.itemValue(`model`, model)
        trace.itemValue(`tokens`, estimateTokens(model, expanded))
        trace.itemValue(`temperature`, temperature)
        trace.itemValue(`top_p`, topP)
        trace.itemValue(`max tokens`, max_tokens)
        trace.itemValue(`seed`, seed)
        trace.itemValue(`response type`, responseType)
        trace.endDetails() // expanded prompt
    }

    trace.endDetails()

    return {
        expanded,
        images,
        schemas,
        functions,
        success,
        model,
        temperature,
        topP,
        max_tokens,
        seed,
        systemText,
        responseType,
        fileMerges,
        outputProcessors,
    }
}
