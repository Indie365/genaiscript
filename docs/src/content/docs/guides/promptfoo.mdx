---
title: Testing with promptfoo
sidebar:
  order: 20
---
import providerSrc from "../../../../../packages/core/src/genaiscript-api-provider.mjs?raw"
import { Code } from '@astrojs/starlight/components';

[promptfoo](https://promptfoo.dev/) is a CLI and library for evaluating LLM output quality.
The guide below shows how to use genaiscript in your promptfoo configurations.

## Configuring

GenAIScript uses a [custom javascript](https://promptfoo.dev/docs/providers/custom-api/) 
provider to integrate with promptfoo. It allows to import the GenAIScript script files (`.genai.js` files)
as prompts in the promptfoo configuration.

- Install [promptfoo](https://www.promptfoo.dev/docs/installation)
- Save `genaiscript-api-provider.mjs` below in your project.
- Add the provider to your `genaiscript.config.yaml` file.

```yaml
providers:
  - id: ./genaiscript/promptfoo-api-provider.mjs
    label: genaiscript:gpt35
    config:
      model: gpt-3.5-turbo
```

- Add the script ids or file path to the `prompts` section

```yaml
prompts:
    - summarize # we have a script with id "summarize.genai.js"
    - ./genaisrc/summarize.genai.js # files work too
```

- Add tests by configuring the files.

```yaml
tests:
    - vars:
          files: src/rag/markdown.md
          transform: output.text
          assert:
            - type: llm-rubric
              value: is a summary
            - type: factuality
              value: Markdown is a text-based syntax to describe documents.

```

- Run `promptfoo` with the configuration file.

```bash
npx promptfoo --config genaiscript.config.yaml
```

### Limitations

Currently, promptfoo treats the script source as the prompt text. Therefore, one cannot use assertions
that also rely on the input text, such as `answer_relevance`.

## Configuration parameters

The provider calls the [cli](/genaiscript/reference/cli) to run the prompt.
The configuration contains various parameters that are converted to command line arguments.

| Parameter | Description |
|--------------------|------------------|
| `model` | The LLM model to use. |
| `temperature` | The temperature to use. |
| `top_p` | The `top p` to use. |
| `cache` | set to `false` to disable cache |
| `version` | The genaiscript version to use if you need to pin it. |

## Output

The output is a JSON object that contains the LLM response 
and all the structured output extracted from it.

## Test vars

The `var.files` can be used to pass the markdown file or files to
the genaiscript provider. It will populate the `env.files` variable in the script.

```yaml
tests:
    - vars:
          files: ./mydocument.md
```

### GenAIScript provider

<Code code={providerSrc} wrap={true} lang="js" title="genaiscript-api-provider.mjs" />

- [GitHub source](https://github.com/microsoft/genaiscript/blob/main/packages/cli/src/genaiscript-api-provider.mjs)