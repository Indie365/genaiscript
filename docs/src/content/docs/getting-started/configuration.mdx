---
title: Configuration
description: Set up your LLM connection and authorization with environment variables for seamless integration.
keywords: LLM setup, API configuration, environment variables, secure authorization, LLM integration
sidebar:
    order: 2
---
import { FileTree } from '@astrojs/starlight/components';
import { Steps } from '@astrojs/starlight/components';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import { Image } from "astro:assets"

import insidersSrc from '../../../assets/vscode-insiders.png'
import insidersAlt from '../../../assets/vscode-insiders.png.txt?raw'

import selectLLMSrc from '../../../assets/vscode-select-llm.png'
import selectLLMAlt from '../../../assets/vscode-select-llm.png.txt?raw'

You will need to configure the LLM connection and authorizion secrets. 

GenAIScript uses a `.env` file to store the secrets.

<Steps>

<ol>

<li>

Create or update a `.gitignore` file in the root of your project and make it sure it includes `.env`.
This ensures that you do not accidentally commit your secrets to your source control.

```txt title=".gitignore" ".env"
...
.env
```

</li>

<li>

Create a `.env` file in the root of your project.

<FileTree>

-  .gitignore
-  **.env**
</FileTree>

</li>

<li>

Update the `.env` file with the configuration information (see below).

</li>

</ol>

</Steps>

To learn more about LLM configurations, 
see [authorization information](/genaiscript/reference/token).

:::caution[Do Not Commit Secrets]

The `.env` file should never be commited to your source control!
If the `.gitignore` file is properly configured, 
the `.env` file will appear grayed out in Visual Studio Code.

:::


### OpenAI

<Steps>

<ol>

<li>
Create a new secret key from the [OpenAI API Keys portal](https://platform.openai.com/api-keys).
</li>

<li>

Update the `.env` file with the secret key.

```txt title=".env"
OPENAI_API_KEY=sk_...
```
</li>

</ol>

</Steps>

### Azure OpenAI ([reference](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions))

<Steps>

<ol>

<li>

Open your [Azure OpenAI resource](https://portal.azure.com) and navigate to the **Keys and Endpoint** tab.

</li>

<li>

Update the `.env` file with the secret key (**Key 1** or **Key 2**) and the endpoint.

```txt title=".env"
AZURE_API_KEY=...
AZURE_OPENAI_ENDPOINT=https://....openai.azure.com
```

</li>

<li>

Update the `model` field in the `script` function to match the model deployment name in your Azure resource.

```js "model"
script({
    model: "deployment-id",
    ...
})
```

</li>

</ol>

</Steps>

### Jan, LMStudio, Ollama

[Jan](https://jan.ai/), [LMStudio](https://lmstudio.ai/), [Ollama](https://ollama.ai/) 
are desktop application that let you run model locally. They expose local API servers 
that compatible with GenAiScript.

Running tools locally may require additional GPU resources depending on the model you are using.

<Steps>

<ol>

<li>

Start Jan/LMStudio/OLLama, load your model and start the **Local API Server**.

</li>

<li>

Update the `.env` file with the local server information.

```txt title=".env"
OPENAI_API_BASE=http://localhost:...
```

</li>

</ol>

</Steps>

### Model specific environment variables

You can provide different environment variables
for each named model by using `MODEL_NAME` prefix.
The model name is capitalized and 
all non-alphanumeric characters are converted to `_`.

This allows to have various sources of LLM computations
for different models. For example, to enable the `ollama:phi3`
model running locally, 
while keeping the default `openai` model connection information.

```txt title=".env"
OLLAMA_PHI3_API_BASE=http://localhost:11434
```

### VSCode Copilot

Visual Studio Code **Insiders** provides an 
**experimental support** to use your Copilot subscription to access a LLM inside of Visual Studio Code.

:::caution[Limitations]

-   **This features requires a GitHub Copilot subscription.**
-   **This feature is still a [proposed api](https://github.com/microsoft/vscode/blob/main/src/vscode-dts/vscode.proposed.languageModels.d.ts) and requires the Insiders editor.**
-   The API changes daily and may not be stable.
-   Functions and Images are not supported.
-   The model configuration, temperature, max tokens, may not be available depending on the language model provider.
-   Some scripts may fail or behave differently due to internal alignment prompts from Copilot.

:::

<Steps>

<ol>

<li>

Install [Visual Studio Code **Insiders**](https://code.visualstudio.com/insiders/). 
It can be installed side-by-side with the stable version of Visual Studio Code.

</li>

<li>

Open your project in Visual Studio Code Insiders (green icon).

<Image src={insidersSrc} alt={insidersAlt} />

</li>

<li>

Download **genaiscript.insiders.vsix** from the [releases page](https://github.com/microsoft/genaiscript/releases/latest)
to your project folder.

<FileTree>

- ...
- **genaiscript.insiders.vsix**

</FileTree>

</li>

<li>

Right click on **genaiscript.insiders.vsix** and select **Install from VSIX**.

</li>

<li>

Add a `.vscode-insiders/argv.json` file to enables loading the proposed APIs for the GenAIScript extension.

```json title=".vscode-insiders/argv.json"
{
    "enable-proposed-api": ["genaiscript.genaiscript-vscode"]
}
```

</li>

<li>

Restart Visual Studio Code Insiders to enable the extension.

</li>

<li>

When you try to run a script, you will be prompted to authorize the Copilot subscription.
After authorizing access, you will see a quick pick dialog to select one of the installed Language Model.

<Image src={selectLLMSrc} alt={selectLLMAlt} />

Note that this dialog is not used once you have a populated `.env` file.

</li>

</ol>

</Steps>

If you are using the Insiders build, make sure to update the **genaiscript.insiders.vsix** regularly
by repeating the steps above. The daily builds of Insiders may contain breaking chagnes that require new builds
of our extension.

## Next steps

Write your [first script](/genaiscript/getting-started/your-first-genai-script).
